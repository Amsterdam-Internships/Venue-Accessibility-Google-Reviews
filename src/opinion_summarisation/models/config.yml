seq2seq_config:
  model_name: "facebook/bart-large"
  tokenizer_name: "facebook/bart-large"
  max_length: 512
  num_beams: 4
  max_summary_length: 150
  batch_size: 8
  learning_rate: 0.00003
  num_epochs: 5
berts:
  bert_config:
    model_name: 'bert-extractive-summarizer'
    batch_size: 8
    learning_rate: 0.00003
  distilbert_config:
    model_name: "distilbert-base-uncased"
    batch_size: 8
    num_epochs: 5
    learning_rate: 0.00003
    hidden: [-1,-2]
    hidden_contact: True
  sbert_config:
    model_name: 'paraphrase-MiniLM-L6-v2'

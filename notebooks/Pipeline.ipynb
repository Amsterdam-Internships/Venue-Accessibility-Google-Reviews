{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files and Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be separated out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "import wordcloud\n",
    "import gensim\n",
    "import scipy.stats as stats\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_iris\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "# Get parent directory\n",
    "parent_dir = os.path.join(current_dir, '..')\n",
    "# Append parent directory to sys.path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aspect_classification.data.data_cleaning import cleaning_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/data/raw/train/EuansGuideData.xlsx\n",
      "path: /Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/data/raw/test/GoogleReviews\n"
     ]
    }
   ],
   "source": [
    "cwd = pathlib.Path.cwd().parent\n",
    "training_file_path = cwd.joinpath(\"data/raw/train/EuansGuideData.xlsx\")\n",
    "test_file_path = cwd.joinpath(\"data/raw/test/GoogleReviews\")\n",
    "print('path:', training_file_path)\n",
    "print('path:', test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_names = glob.glob(str(test_file_path) + \"/*.csv\")\n",
    "google_df = [pd.read_csv(file_name, index_col=None, header=0) for file_name in all_file_names]\n",
    "test_data = pd.concat(google_df, axis=0, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/notebooks/../src/aspect_classification/data/data_cleaning.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Text\"] = df[\"Text\"].apply(lambda x: x.replace(\"\\n\", ' '))\n",
      "/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/notebooks/../src/aspect_classification/data/data_cleaning.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_aspects[\"Venue\"] = selected_aspects[\"Venue\"].apply(get_venue_name)\n",
      "/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/notebooks/../src/aspect_classification/data/data_cleaning.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Sentiment\"] = df[\"Rating\"].apply(lambda x : convert_score(x))\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_excel(training_file_path)\n",
    "clean_train_df = cleaning_selector(training_data, [\"Aspect\", \"Rating\", \"Review\", \"Venue\"])\n",
    "clean_test_df = cleaning_selector(test_data, [\"Name\",\"Review Rate\", \"Review Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation+'\\u2026')\n",
    "\n",
    "def remove_small_words(sentence):\n",
    "   \n",
    "    if len(nltk.word_tokenize(sentence.translate(translator))) >= 5:\n",
    "       return sentence\n",
    "    else:\n",
    "       return \"useless\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'\\(Translated by Google\\)|\\(Original\\)'  # Define the regular expression pattern\n",
    "clean_test_df['Text'] = clean_test_df['Text'].str.replace(pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_df[400:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = clean_test_df.assign(text=clean_test_df['Text'].apply(nltk.sent_tokenize)).explode('Text').reset_index(drop=True)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.explode('text').reset_index(drop=True)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.drop('Text', axis=1)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['text'] = sample_df['text'].apply(remove_small_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df[sample_df['text'].str.contains(\"useless\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pattern = r\".*([Pp]ancakes|[Dd]rink|[Dd]esserts|[Gg]in|[Ww]ine|[Bb]reakfast|[Ll]unch|[Pp]asta|[Vv]egeterian|[Vv]egan|[Bb]urgers|[Pp]asta|[Dd]ish|[Bb]eer|[Pp]izza|[Tt]aste|[Ff]ood|[Cc]ocktail|[Cc]offee|[Mm]enu|[Tt]asty|[Dd]elicious|[Ss]taff|[Hh]ost|[Aa]mbience|[Aa]tmosphere|[Cc]o[sz]y|[Gg]ezellig|[Ss]ervice]|[Pp]rice[y]|[Cc]heap|([Nn]ice|[Gg]reat|[Aa]mazing) place|([Gg]ood|[Bb]ad|[Tt]errible|[Gg]reat) experience).*\"\n",
    "\n",
    "mask = sample_df['text'].str.contains(reg_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "\n",
    "# # define a function to check if a word is related to food or drink\n",
    "# def is_food_or_drink(word):\n",
    "#     food_synsets = wn.synsets('food')\n",
    "#     drink_synsets = wn.synsets('drink')\n",
    "#     food_and_drink_words = set()\n",
    "#     for synset in food_synsets + drink_synsets:\n",
    "#         for lemma in synset.lemmas():\n",
    "#             food_and_drink_words.add(lemma.name())\n",
    "#     other_food_and_drink_words = {'breakfast', 'lunch', 'dinner', 'alcohol'}\n",
    "#     food_and_drink_words |= other_food_and_drink_words\n",
    "#     return word in food_and_drink_words\n",
    "\n",
    "# # identify the rows that contain food or drink related words\n",
    "# food_rows = []\n",
    "# for i, row in sample_df.iterrows():\n",
    "#     for word in nltk.word_tokenize(row['text']):\n",
    "#         if is_food_or_drink(word):\n",
    "#             food_rows.append(i)\n",
    "\n",
    "# # drop the rows that contain food or drink related words\n",
    "# sample_df.drop(food_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_random_sample = sample_df.sample(frac=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((google_random_sample).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_random_sample.to_excel('/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/datasets/labelling data/improved2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_excel('/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/datasets/11random_google_reviews_excel.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named df\n",
    "search_terms = ['Parking', 'Transport', 'Toilets', 'Access', 'Entrance', 'Accessibility', 'Wheelchair', 'Staff']\n",
    "num_rows = sample_df[sample_df['text'].str.contains('|'.join(search_terms), case=False, na=False)].shape[0]\n",
    "print(f\"There are {num_rows} rows that mention the search terms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white', max_words=1000, contour_width=3, contour_color='steeleblue')\n",
    "\n",
    "clustered_reviews_train = ','.join(list(clean_train_df['Text'].values))\n",
    "clustered_reviews_test = ','.join(list(clean_test_df['Text'].values))\n",
    "wordcloud.generate(clustered_reviews_train)\n",
    "wordcloud.generate(clustered_reviews_test)\n",
    "wordcloud.to_image()\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_per_aspect = clean_train_df.groupby(['Aspect']).count()\n",
    "amount_per_aspect = amount_per_aspect['Text']\n",
    "amount_per_aspect.plot(kind='bar', title=\"Overview of Aspects in Euan's Guide data\", ylabel='Amount of aspects', xlabel='Aspect Types', figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_per_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_per_sentiment = clean_train_df.groupby(['Sentiment']).count()\n",
    "amount_per_sentiment = amount_per_sentiment['Text']\n",
    "amount_per_sentiment.plot(kind='bar', title=\"Overview of Sentiments in Euan's Guide data\", ylabel='Amount of Each Sentiment', xlabel='Sentiment Types', figsize=(6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_per_sentiment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "n = round(len(clean_train_df))\n",
    "euans_reviews = clean_train_df.Text.values.tolist()\n",
    "google_reviews = clean_test_df[:n].Text.values.tolist()\n",
    "euans_labels = clean_train_df.Aspect.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('clf', VotingClassifier([\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "    'vectorizer__ngram_range': ((1, 1), (1, 2)),\n",
    "    'clf__voting': ('soft', 'hard'),\n",
    "    'clf__nb__alpha': (0.5, 1),\n",
    "    'clf__lr__C': (0.1, 1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=5, n_jobs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(euans_reviews, euans_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "google_sample = random.sample(google_reviews, len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_val)\n",
    "print(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_pred = grid_search.predict(google_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_labels = ['Toilets', 'Transport & Parking']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuation_report = classification_report(y_val, y_pred, labels=pos_labels)\n",
    "evaluation_report = classification_report(y_val, google_pred, labels=pos_labels)\n",
    "print(\"Euan's Guide Evaluation Report\\n\",valuation_report)\n",
    "print(\"Google Reviews Evaluation Report\\n\",evaluation_report)\n",
    "# save report as a text file\n",
    "with open('../Results/google_aspect_classification_report.txt', 'w') as f:\n",
    "    f.write(evaluation_report)\n",
    "    \n",
    "with open('../Results/euans_aspect_classification_report.txt', 'w') as f:\n",
    "    f.write(valuation_report)\n",
    "\n",
    "# # convert text file to PNG image\n",
    "img = Image.new('RGB', (800, 800), color='white')\n",
    "font = ImageFont.truetype('../media/Fonts/Roboto/Roboto-Black.ttf', 20)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "with open('../Results/google_aspect_classification_report.txt', 'r') as f:\n",
    "    y = 0\n",
    "    for line in f.readlines():\n",
    "        draw.text((10, y), line, fill='black', font=font)\n",
    "        y += 20\n",
    "\n",
    "img.save('../Results/google_aspect_classification_report.png')\n",
    "\n",
    "with open('../Results/euans_aspect_classification_report.txt', 'r') as f:\n",
    "    y = 0\n",
    "    for line in f.readlines():\n",
    "        draw.text((10, y), line, fill='black', font=font)\n",
    "        y += 20\n",
    "\n",
    "img.save('../Results/euans_aspect_classification_report.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_val, google_pred)\n",
    "\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pos_labels)\n",
    "\n",
    "display.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = clean_train_df.Sentiment.values.tolist()\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(euans_reviews, sentiment_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('clf', VotingClassifier([\n",
    "        ('nb', MultinomialNB()),\n",
    "        ('svm', SVC())\n",
    "    ]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_params = {\n",
    "    'vectorizer__max_features': [1000, 5000],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__voting': ['hard', 'soft'],\n",
    "    'clf__weights': [[0.5, 0.5], [0.7, 0.3]],\n",
    "    'clf__estimators': [\n",
    "        [('nb', MultinomialNB(alpha=0.5)), ('svm', SVC(kernel='linear', C=1.0))],\n",
    "        [('nb', MultinomialNB(alpha=1.0)), ('svm', SVC(kernel='rbf', C=10.0, gamma=0.1))]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=sentiment_params, cv=5, n_jobs=5, verbose=1)\n",
    "grid_search2.fit(X2_train, y2_train)\n",
    "y2_pred = grid_search2.predict(X2_val)\n",
    "google2_pred = grid_search2.predict(google_sample)\n",
    "euans_report = classification_report(y2_val, y2_pred)\n",
    "google_report = classification_report(google2_pred, y2_pred)\n",
    "\n",
    "print('Euans Report\\n', euans_report)\n",
    "print('Google Report\\n', google_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save report as a text file\n",
    "with open('../Results/google_sentiment_analysis_report.txt', 'w') as f:\n",
    "    f.write(google_report)\n",
    "    \n",
    "with open('../Results/euans_sentiment_analysis_report.txt', 'w') as f:\n",
    "    f.write(euans_report)\n",
    "\n",
    "# # convert text file to PNG image optimise this as you repeat this code.\n",
    "img = Image.new('RGB', (800, 800), color='white')\n",
    "font = ImageFont.truetype('../media/Fonts/Roboto/Roboto-Black.ttf', 20)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "with open('../Results/euans_sentiment_analysis_report.txt', 'r') as f:\n",
    "    y = 0\n",
    "    for line in f.readlines():\n",
    "        draw.text((10, y), line, fill='black', font=font)\n",
    "        y += 20\n",
    "\n",
    "img.save('../Results/euans_sentiment_analysis_report.png')\n",
    "\n",
    "with open('../Results/google_sentiment_analysis_report.txt', 'r') as f:\n",
    "    y = 0\n",
    "    for line in f.readlines():\n",
    "        draw.text((10, y), line, fill='black', font=font)\n",
    "        y += 20\n",
    "\n",
    "img.save('../Results/google_sentiment_analysis_report.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y2_val, google2_pred)\n",
    "\n",
    "display2 = ConfusionMatrixDisplay(confusion_matrix=cm2, display_labels=pos_labels)\n",
    "\n",
    "display2.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion Summarisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moved this part to Google Colab https://colab.research.google.com/drive/1NVzQ3vS6oaQ7EPFzzij1XbjDQT0QpOBO?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_test_df.to_csv('/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/datasets/google_reviews.csv', index=False)\n",
    "#clean_train_df.to_csv(\"/Users/mylene/BachelorsProject/Venue-Accessibility-Google-Reviews/datasets/euans_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# from textblob import TextBlob\n",
    "# summariser = pipeline('summarization', model='distilbert-base-uncased')\n",
    "\n",
    "# \"\"\"\n",
    "# We want the review text per venue, aspect and sentiment.\n",
    "# \"\"\"\n",
    "# # Possibly vader could look at identifying the sentiment correctly.\n",
    "# # [['Venue', 'Aspect', 'Sentiment', 'Text']]\n",
    "\n",
    "\n",
    "# summaries = summariser(euans_reviews, max_length=50, min_length=10)\n",
    "\n",
    "# reviews_per_venue = clean_train_df['summary'] = [summary['Summarised Review'] for summary in summaries]\n",
    "# reviews_per_venue.head()\n",
    "\n",
    "\n",
    "# # Group all reviews per venue\n",
    "# # generate a summary of the sentiment and aspects for that venue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lex Rank implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "long_reviews = clean_train_df[clean_train_df['SentenceCount'] > 1]\n",
    "long_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "i = 0\n",
    "for review in long_reviews.Text.values.tolist():\n",
    "    parser = PlaintextParser.from_string(review,Tokenizer('english'))\n",
    "    i+=1\n",
    "    lex_rank_summarizer = LexRankSummarizer()\n",
    "    summaries.append(lex_rank_summarizer(parser.document, sentences_count=1)) \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_empty_summary(summary):\n",
    "    if len(summary) == 0:\n",
    "        return \"summary processing error\"\n",
    "    else:\n",
    "        return str(summary[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_reviews['Lex Rank Summary'] = summaries\n",
    "long_reviews['Lex Rank Summary'] = long_reviews['Lex Rank Summary'].apply(lambda x: find_empty_summary(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_reviews['Lex Rank Summary']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "lsa_summarizer=LsaSummarizer()\n",
    "lsa_summaries = []\n",
    "for review in long_reviews.Text.values.tolist():\n",
    "    lsa_parser=PlaintextParser.from_string(review,Tokenizer('english'))\n",
    "    lsa_summaries.append(lsa_summarizer(lsa_parser.document,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_reviews['LSA Summaries'] = lsa_summaries\n",
    "long_reviews['LSA Summaries'] = long_reviews['LSA Summaries'].apply(lambda x: find_empty_summary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_rank_summaries = long_reviews['Lex Rank Summary'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_summaries = long_reviews['LSA Summaries'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_processed = [dc.preprocess(review) for review in lex_rank_summaries]\n",
    "\n",
    "lsa_processed = [dc.preprocess(review) for review in lsa_summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_dict = gensim.corpora.Dictionary(lsa_processed)\n",
    "count = 0\n",
    "for k, v in topic_dict.iteritems():\n",
    "    print(k, v)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_corpus = [topic_dict.doc2bow(doc) for doc in lsa_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(lsa_corpus)\n",
    "lsa_corpus_tfidf = tfidf[lsa_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(lsa_corpus, num_topics=10, id2word=topic_dict, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(lsa_corpus_tfidf, num_topics=10, id2word=topic_dict, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on google reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_df['Sentence Count'] = clean_test_df['Text'].apply(lambda x: dc.count_sentences(x))\n",
    "long_google_reviews = clean_test_df[clean_test_df['Sentence Count'] > 1]\n",
    "long_google_reviews = long_google_reviews[:len(y_val)]\n",
    "long_google_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lsa_summary(review):\n",
    "    google_lsa_parser=PlaintextParser.from_string(review,Tokenizer('english'))\n",
    "    return lsa_summarizer(google_lsa_parser.document,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_google_reviews['LSA Summary'] = long_google_reviews['Text'].apply(lambda x: create_lsa_summary(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_google_reviews['LSA Summary'] = long_google_reviews['LSA Summary'].apply(lambda x: find_empty_summary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_lsa = long_google_reviews['LSA Summary'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(google_lsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_lsa_processed = [dc.preprocess(review) for review in google_lsa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_dict = gensim.corpora.Dictionary(google_lsa_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_dict.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_bow_corpus = [google_dict.doc2bow(doc) for doc in google_lsa_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[google_bow_corpus[10]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf[google_bow_corpus[10]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis_train = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=lsa_corpus, dictionary=topic_dict)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_test = pyLDAvis.gensim.prepare(topic_model=lda_model_tfidf, corpus=google_bow_corpus, dictionary=google_dict)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do a bag of words model or something else\n",
    "2. Do this for each summary and normal text\n",
    "3. Generate a topic distribution with LDA on both generated summary and normal text\n",
    "4. Evaluate with Kullback-Leibler and Jensen-Shannon divergence or cosine similarity\n",
    "\n",
    "5. create ground truth from BERT model \n",
    "6. compare this to the LSA and Lex Rank summaries with cosine similarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Lex and LSA summaries for the google reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=\"english\")\n",
    "not_summarised = long_google_reviews.Text.values.tolist()\n",
    "cosine_scores = []\n",
    "for review_a, review_b in zip(google_lsa, not_summarised):\n",
    "\n",
    "    lsa_goole_vec = vectorizer.fit_transform([review_a])\n",
    "    normal_google_vec = vectorizer.transform([review_b])\n",
    "    cosine_sim = cosine_similarity(lsa_goole_vec, normal_google_vec)[0][0]\n",
    "    cosine_scores.append(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_differences = []\n",
    "for i in range(len(not_summarised)):\n",
    "    length_difference = len(not_summarised[i]) - len(google_lsa[i])\n",
    "    length_differences.append(length_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(cosine_scores, length_differences)\n",
    "fig = plt.figure(num='Summarisation stats')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Difference in Length')\n",
    "plt.title('Relationship between Cosine Similarity and Difference in Length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
